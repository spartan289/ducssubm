{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1731c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import torch\n",
    "\n",
    "def read_jsonl(file_path, nrows=None):\n",
    "    return pd.read_json(file_path, lines=True, nrows=nrows)\n",
    "\n",
    "\n",
    "train_data = read_jsonl('./train.features')\n",
    "train_solution = read_jsonl('./train.labels')\n",
    "test_data = read_jsonl('./final_test_data.features')\n",
    "#test_solution = read_jsonl('./data/attribute_test.solution', nrows=200)\n",
    "#val_data = read_jsonl('C:/Users/Administrator.DUCS-GPU/Desktop/data/attribute_val.data')\n",
    "#val_solution = read_jsonl('C:/Users/Administrator.DUCS-GPU/Desktop/data/attribute_val.solution')\n",
    "\n",
    "def preprocess_data(data, solution=None):\n",
    "    if solution is not None:\n",
    "        merged = pd.merge(data, solution, on='indoml_id')\n",
    "        merged['input_text'] = merged.apply(lambda row: f\"description: {row['description']} retailer: {row['retailer']} price: {row['price']}\", axis=1)\n",
    "        merged['target_text'] = merged.apply(lambda row: f\"supergroup: {row['supergroup']} group: {row['group']} module: {row['module']} brand: {row['brand']}\", axis=1)\n",
    "        return merged[['input_text', 'target_text']]\n",
    "    \n",
    "    else:\n",
    "        data['input_text'] = data.apply(lambda row: f\"description: {row['description']} retailer: {row['retailer']} price: {row['price']}\", axis=1)\n",
    "        return data[['input_text']]\n",
    "\n",
    "# Assuming train_data and train_solution are in the desired format\n",
    "train_data, val_data, train_solution, val_solution = train_test_split(train_data, train_solution, test_size=0.3, random_state=42)\n",
    "\n",
    "## Save DataFrames to .txt files\n",
    "#train_data.to_csv('train_data.txt', sep='\\t', index=False)\n",
    "#val_data.to_csv('val_data.txt', sep='\\t', index=False)\n",
    "#\n",
    "## Save solutions to .txt files\n",
    "#train_solution.to_csv('train_solution.txt', sep='\\t', index=False, header=False)\n",
    "#val_solution.to_csv('val_solution.txt', sep='\\t', index=False, header=False)\n",
    "#\n",
    "train_processed = preprocess_data(train_data, train_solution)\n",
    "test_processed = preprocess_data(test_data)\n",
    "val_processed = preprocess_data(val_data, val_solution)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_processed)\n",
    "test_dataset = Dataset.from_pandas(test_processed)\n",
    "val_dataset = Dataset.from_pandas(val_processed)\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    #'test': test_dataset,\n",
    "   'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f376e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    model_inputs = tokenizer(inputs, max_length=128, padding='max_length', truncation=True)\n",
    "    labels = tokenizer(targets, max_length=128, padding='max_length', truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.save_to_disk('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0379c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_datasets = load_from_disk('./')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=100,\n",
    "    per_device_eval_batch_size=100,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            print(f\"Step: {state.global_step}\")\n",
    "            for key, value in logs.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    callbacks=[CustomCallback()],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b9f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "val_results = trainer.evaluate(eval_dataset=tokenized_datasets['validation'])\n",
    "print(f\"Validation Loss: {val_results['eval_loss']}\")\n",
    "\n",
    "#test_results = trainer.evaluate(eval_dataset=tokenized_datasets['test'])\n",
    "#print(f\"Test Loss: {test_results['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b59a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./1st')\n",
    "tokenizer.save_pretrained('./1st')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ceea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('./1st').to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained('./1st')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_data = test_dataset['input_text']\n",
    "#test_label = test_dataset['target_text']\n",
    "\n",
    "def generate_text(inputs):\n",
    "    inputs = tokenizer.batch_encode_plus(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=400)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=256)\n",
    "    \n",
    "    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    #cleaned_texts = [clean_repeated_patterns(text) for text in generated_texts]\n",
    "    return generated_texts\n",
    "\n",
    "def extract_details(text):\n",
    "    pattern = r'supergroup: (.*?) group: (.*?) module: (.*?) brand: (.*)'\n",
    "    match = re.match(pattern, text)\n",
    "    if match:\n",
    "        return tuple(item if item is not None else 'na' for item in match.groups())\n",
    "    return 'na', 'na', 'na', 'na'\n",
    "\n",
    "def clean_repeated_patterns(text):\n",
    "    #cleaned_data = text.split(' L4_category')[0] \n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3500\n",
    "generated_details = []\n",
    "target_details = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_data), batch_size), desc=\"Processing test data\"):\n",
    "    batch_inputs = test_data[i:i+batch_size]\n",
    "    #batch_labels = test_label[i:i+batch_size] #you are not going to have this\n",
    "    \n",
    "    generated_texts = generate_text(batch_inputs)\n",
    "    \n",
    "    for generated_text in generated_texts:\n",
    "        generated_details.append(extract_details(generated_text))\n",
    "\n",
    "print('Generated info extracted.............')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "categories = ['supergroup', 'group', 'module', 'brand']\n",
    "\n",
    "with open('1st.predict', 'w') as file:\n",
    "\n",
    "    for indoml_id, details in enumerate(generated_details):\n",
    "        result = {\"indoml_id\": indoml_id}\n",
    "        for category, value in zip(categories, details):\n",
    "            result[category] = value\n",
    "        \n",
    "        file.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26713e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "file_to_zip = '1st.predict'\n",
    "zip_file_name = '1st.zip'\n",
    "\n",
    "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
    "    zipf.write(file_to_zip, arcname=file_to_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8385ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
